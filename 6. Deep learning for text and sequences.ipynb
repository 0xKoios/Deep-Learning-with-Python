{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6. Deep learning for text and sequences.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNqe8NU1wRIMsVIIkqNCtr7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jetsukda/Deep-Learning-with-Python/blob/main/6.%20Deep%20learning%20for%20text%20and%20sequences.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNTz1W8SxlS4"
      },
      "source": [
        "**This chapter covers**\n",
        "- Preprocessing text data info useful representations\n",
        "- Working with recurrent neural networks\n",
        "- Using 1D convnets for sequence processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZIV1LKoyBTW"
      },
      "source": [
        "# 6.1 Working with text data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fo0keeOf5Oan"
      },
      "source": [
        "**Text is one of the most widespread forms of sequence data.**\n",
        "\n",
        "- Sequence of characters.\n",
        "- Sequence of words.\n",
        "- *but it's most common to work at the level of words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSI4h3zcAr0V"
      },
      "source": [
        "**The deep-learning sequence-processing models introduced in the following sections can use text to produce a basic from of natural language understanding**\n",
        "- Document classification\n",
        "- Sentiment analysis\n",
        "- Author identification\n",
        "- Question answering (QA)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPKa6ZhqAuZh"
      },
      "source": [
        "**Natural language processing (NLP)**\n",
        "- Deep learning for natural language processing is pattern recognition applied to words, sentences, and paragraphs, in much the same way that computer vision is pattern recognition applied to pixels.\n",
        "- Deep learning models don't take as input raw text: they only work with numeric tensors. \n",
        "- ***Vectorizing*** is the process of transforming text into numeric tensors\n",
        "- This can be done in multiple ways:\n",
        "    - Segment text into words, and transform each word into a vector.\n",
        "    - Segment text into characters, and transform each character into a vector.\n",
        "    - Extract **N-grams** of **words** or **character**, and transform each n-gram into vector.\n",
        "    - ***N-grams*** are overlapping groups of multiple consecutive words or characters.\n",
        "- **Words, character, or n-grams** are called ***tokens***.\n",
        "- Breaking text into such tokens is called ***tokenization***.\n",
        "- All text-vectorization processes consist of applying some tokenization scheme and then associating numeriv vectors with the generated tokens.\n",
        "- These vecctors, packed into **sequence tensors**\n",
        "- There are multiple ways to associate a  vector with a token.\n",
        "    - one-hot encoding\n",
        "    - token embedding (word embedding)\n",
        "\n",
        "<p align=\"center\">\n",
        "        <img src=\"https://drive.google.com/uc?export=view&id=1iSf4bn8xyBRpFFIk1Nr1f_PHDzq9-WcC\" width=\"700\">\n",
        "        </p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DknR1MdZRNT"
      },
      "source": [
        "## 6.1.1 One-hot Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTP0SOPqZbfq"
      },
      "source": [
        "- It consists of associating a unique index with every word and then turning this integer index `i` into a binary vector of size `N`. (the size of the vocabulary)\n",
        "- The vector is all zeros except for the `i`'th entry, which is 1.\n",
        "- one-hot encoding can be done at and how to implement it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sojv_cKnOX1u"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nNaQKqTArGH"
      },
      "source": [
        "# Using Keras for word-level one-hot encoding\n",
        "samples = [\"The cat sat on the mat.\", \"The dog ate my homework.\"]\n",
        "\n",
        "# Creates a tokenizer\n",
        "# Configured to only take into account the 1,000 most common words\n",
        "tokenizer = Tokenizer(num_words=1000)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLCQUwxFPcld",
        "outputId": "ebca643a-dd9d-47d4-91b6-20578890bbd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Builds the word index\n",
        "tokenizer.fit_on_texts(samples)\n",
        "print(\"{index:word}->\",tokenizer.index_word)\n",
        "print(\"{word:index}->\",tokenizer.word_index)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{index:word}-> {1: 'the', 2: 'cat', 3: 'sat', 4: 'on', 5: 'mat', 6: 'dog', 7: 'ate', 8: 'my', 9: 'homework'}\n",
            "{word:index}-> {'the': 1, 'cat': 2, 'sat': 3, 'on': 4, 'mat': 5, 'dog': 6, 'ate': 7, 'my': 8, 'homework': 9}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDKebYKwPogu",
        "outputId": "8f6dfdb5-689c-4e18-dc82-db222909e8e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Turns strings into lists of integer indices\n",
        "sequences = tokenizer.texts_to_sequences(samples)\n",
        "for sentence, sequence in zip(samples, sequences):\n",
        "    print(\"Sentence:\", sentence)\n",
        "    print(\"Sequence:\", sequence)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence: The cat sat on the mat.\n",
            "Sequence: [1, 2, 3, 4, 1, 5]\n",
            "Sentence: The dog ate my homework.\n",
            "Sequence: [1, 6, 7, 8, 9]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_F5GtbARIct",
        "outputId": "3c95fdd2-750d-4b5d-cdd9-88222470bc9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# one-hot encoding\n",
        "one_hot_results = tokenizer.texts_to_matrix(samples, mode=\"binary\")\n",
        "one_hot_results"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 1., ..., 0., 0., 0.],\n",
              "       [0., 1., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrIt-_vYaPCx"
      },
      "source": [
        "## 6.1.2 Using word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpwK9buqaUoS"
      },
      "source": [
        "- Popular and powerful way to associate a vector with a word\n",
        "- **Dense word vectors** also called **word embeddings**.\n",
        "- **One-hot encodding** : **binary**, **sparse** (mostly made of zeros), and **ver high-dimensional** (same dimensionality as the number of words in the vocabulary)\n",
        "- **Word embedding** : **low dimensional floating-point vectors** (that is, dense vectors, as opposed to sparse vectors)\n",
        "- **Word embedding** are learn form data.\n",
        "    - It's common to see word embedding that are 256-dimensional, 512-dimensional, or 1,024-dimensional when dealing with very large vocabularies.\n",
        "- **One-hot encoding** words generally leads to vectors that are 20,000-dimensional or greater (capturing a vocabbulary of 20,000 tokens)\n",
        "- **Word embeddings** pack more information into far fewer dimensions.\n",
        "\n",
        "<p align=\"center\">\n",
        "        <img src=\"https://drive.google.com/uc?export=view&id=1edEiY05tRr2Z7vQcl4WVCKCPr4yRsekE\" width=\"700\" >\n",
        "        </p>\n",
        "\n",
        "- There are two ways to obtain word embeddings:\n",
        "    - Learn word embeddings jointly with the main task you care about\n",
        "        - Document classification\n",
        "        - Sentiment prediction\n",
        "        - *In this setup, you start with random word vectors and then learn word vectors in the same way you learn the weights of neural network.\n",
        "    - Load into your model word embeddings that were precomputed using a different machine-learning task than the one you're trying to solve.\n",
        "        - These are called **pretrained word embeddings**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gWjYcXruR4u"
      },
      "source": [
        "**Learning word embeddings with the embedding layer**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSt-aeLeyHIn"
      },
      "source": [
        "# 6.2 Understanding recurrent neural networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ye7-JcadyN4Q"
      },
      "source": [
        "# 6.3 Advanced use of recurrent neural networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2o-EwYmyUY_"
      },
      "source": [
        "# 6.4 Sequence processing with convnets"
      ]
    }
  ]
}