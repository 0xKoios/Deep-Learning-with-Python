{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3. Getting started with neural networks.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOMk4PHWm3fOk0KuRxG61Yj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jetsukda/Deep-Learning-with-Python/blob/main/3.%20Getting%20started%20with%20neural%20networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KgbAUZE-uZ1"
      },
      "source": [
        "> This chapter cover\n",
        "- Core components of neural networks\n",
        "- An introduction of Keras\n",
        "- Setting up a deep-learning workstation\n",
        "- Using neural networks to solve basic classification and regression problems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUD-ixB4_Yrk"
      },
      "source": [
        "# 3.1 Anatomy of a neural network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esmGPNeQ_cXm"
      },
      "source": [
        "> Learning a neural network revolves around the folwing object:\n",
        "- ***Layers***, which are combined into a ***network*** (or ***model***)\n",
        "- The ***input data*** and corresponding ***targets**\n",
        "- The ***loss function***, which defines the feedback signal used for learning\n",
        "- The ***optimizer***, which determines how learning proceeds\n",
        "\n",
        "\n",
        "<p align=\"center\">\n",
        "        <img src=\"https://drive.google.com/uc?export=view&id=1_QSSAYQ4kUAixQi9Anw4ilWSpjVLEJjo\" width=\"700\" >\n",
        "        </p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwyV3SQZCcqU"
      },
      "source": [
        "## 3.1.1 Layers: the building blocks of deep learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZtwDig0WPGk"
      },
      "source": [
        "> The fundamental data structure in neural networks is the ***layer***.\n",
        "- A layer is a **data-processing** module that takes as input one or more tensors and that outputs one or more tensors.\n",
        "- Some layers are stateless, but more frequently layers have a state\n",
        "- The layer's ***weights***, one or sevral tensors learned with SGD, which together contain **the network's knowledge**.\n",
        "\n",
        "> **Different layers** are **appropriate for different tensor formats** and **different types of data processing**.\n",
        "\n",
        "**For instance**\n",
        "- **Simple vector data (2D tensors)** : (samples, features) is often processed by ***densely connected*** layers, also called ***fully connected*** or ***dense*** layers (the `Dense` class in Keras).\n",
        "- **Sequence data (3D tensors)** : (samples, timesteps, features), is typically processed by recurrent layers such as an `LSTM` layers.\n",
        "- **Image data** : stored in 4D tensors, is usually processed by 2D convolution layers (`Conv2D`)\n",
        "\n",
        "***You can think of layers as the LEGO bricks of deep learning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5y4eXLzak6R"
      },
      "source": [
        "- Building deep-learning models in Keras is done by clipping together compatible layers to form useful data-transformation pipelines.\n",
        "\n",
        "- The notion of ***layer compatibility*** here refers specifically to **the fact** that every layer will only **accept input tensors of a certain shape** and **will return output tensors of a certain shape**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OIE3yDyakmC"
      },
      "source": [
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-k6ncAa64Fu"
      },
      "source": [
        "# A dense layer with 32 output units\n",
        "layer = layers.Dense(32, input_shape=(784,))"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3qaT27-bwOV"
      },
      "source": [
        "We're creating a layer that will only accept as input **2D tensors** where the first dimension is 784 (axis 0, the batch dimension, is unspecified, and thus any value would be accepted).\n",
        "\n",
        "- This layer will return a tensor where the first dimension has been transformed to be 32."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gLbxUv6cKos"
      },
      "source": [
        "# connected to a downstream layer expects 32 dimensional vectors as its input.\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(32, input_shape=(784,)))\n",
        "model.add(layers.Dense(32))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dKq7IrodOfA"
      },
      "source": [
        "The second layer didn't receive an input shape argument\n",
        "- Instead, **it automatically inferred its input shape as being the output shape of the layer that came before**(the first layer)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihNOcPswdBn8"
      },
      "source": [
        " "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}